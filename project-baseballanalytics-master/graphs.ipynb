{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Sigh of Relief: Taking the Guesswork out of Pitching Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Since the introduction of Sabermetrics in 1974, Major League Baseball has remained at the forefront of statistical analysis in professional sports.  Given the ease of access to extremely revealing data, teams rely on analytics to make both in and out of game decisions.  For managers, the most significant of these decisions is choosing when to pull their starting pitcher.  \n",
    "\n",
    "    We set out to develop a model that could, in the place of a manager, decide whether or not to remove the starting pitcher from a given game for a given pitch.  The model is practically useful because managers are often thrown out of games, forcing the pitching coach to make personnel decisions in lieu of the manager.  These decisions don't always align with that of the manager, so our model could be used to help these coaches avoid the embarrassment of a bad decision.  Also, our model could be used by the hitting team in order to make pinch hitting decisions.  By accurately predicting when a pitcher will be pulled, a manager can make more informed substitutions and ensure that they utilize their pinch hitters with maximum efficiency.\n",
    "\n",
    "    Luckily, we were able to find a richly detailed dataset that included over 50 data points for each pitch for the 2014, 2015, and 2016 regular seasons.  Variables included the state of the game (score, pitcher, batter, baserunners, etc) and various pitch-by-pitch metrics such as release point, pitch location, and the result of each pitch.  We focused specifically on the game state -- and developed a model using a decision tree that can predict whether or not a pitcher will be pulled from a game after a given pitch with up to 99.65% accuracy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    In order to build our model, we first had to decide exactly who we were going to focus on.  Originally, we sought out to develop a model that could predict when any pitcher is pulled.  However, we eventually realized that relief pitching is too hectic and unpredictable to fit a reasonable model to.  Thus, we decided to remove all relievers from our dataset.  Since one of the few things the original dataset did not include was position, we had to write methods that determined which pitchers were starting games, which we wrote to a new file called ‘startersOnly{year}.csv’ (code in cleanRelievers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cleanRelievers\n",
    "# do clean relievers\n",
    "cleanRelievers('2016.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    After removing relievers, we had to begin thinking about which features we wanted to include in our model.  Originally, we tried the model on a set of variables already in the dataset, such as the score, release velocity, and result of the last pitch, but our results weren’t much better than a null model.  This was because each pitch only contains the result of that pitch -- it doesn’t take into account cumulative totals, such as hit count, walk count, runs given up, and pitch count (metrics which all managers consider when deciding whether or not to pull a pitcher).  So we decided to rewrite the file with a handful of new, added variables.  We added 15 new fields to our dataset, all of which were simple data processing tasks on the starters only version of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add new fields"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    We initially examined quality of pitch data, such as release velocity, spin rate, and the probability of being called a strike.  Surprisingly, there was little correlation between these metrics and whether or not a pitcher would be pulled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tableau graphs\n",
    "<img src = \"game pitch count.jpg\">\n",
    "<img src = \"week and season pitch count.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we had all of our data parsed and our new fields added, we visualized the data using matplotlib in order determine the effect that a given feature had on the likelihood of a pitcher being pulled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do initialize_graphs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Finally, we were ready to implement our model and test our predictions.  Given the high infogain on a few features such as middle_at_bat, end_inning, and pitch_number, we decided that a decision tree would produce the most accurate predictions.  Using the new fields we added as features, we modified our code to work with the decision tree we implemented in lab7.  We also implemented a naive bayes and linear regression model using sci-kit learn, and were able to increase our accuracy by a small amount.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do tree.py and machine_learn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = \"Results Table.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"final model accuracy.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Our decision tree is able to predict with 99.65% accuracy whether a pitch will be a pitcher’s last pitch for that game on the dataset which includes all pitches.  Since the vast majority of pitches are not a pitcher’s last pitch, the naive assumption of always classifying a pitch as not the last pitch is also very accurate, predicting 98.95% of pitches correctly.  Our model is a significant improvement on the always false assumption; we misidentify one-third as many pitches as the always false assumption does.  \n",
    "    \n",
    "\tSince the vast majority of pitches are not a pitcher’s last pitch, we also narrowed the dataset to only include pitches from the 6th inning and on, when pitchers are more likely to be pulled.  On this constrained dataset our decision tree predicts with 98.17% accuracy whether a pitch will be a pitcher’s last for that game.  This is an even bigger improvement over the always false assumption which is accurate 95.78% of the time. \n",
    "    \n",
    "\tIn addition to creating a model based off of all pitcher data which predicts whether a pitch will be a pitcher’s last, we also can create individual models for each specific pitcher.  These models are more accurate than the general model and on average identify 99.98% of pitches accurately -- an improvement of .3% over the general model and significantly better than the always false assumption which identifies 98.9% accurately.  This model misidentifies roughly one-third as many pitches as the general model and around one-tenth as many pitches as the always false assumption.  \n",
    "    \n",
    "\tIn addition to our decision tree, we also created naive bayes and linear regression models to predict whether a pitch will be the last pitch of the game for a starting pitcher.  The results are shown above in table 1.  Neither of these models are as accurate as the decision tree and they are both more reliant on including the end_inning field.  In fact, without the end_inning field, the naive bayes model performs worse than always classifying a pitch as not the last pitch.  \n",
    "    \n",
    "\tOther than our models which predict whether a given pitch will be a pitcher’s last, our main discovery is that the most important determinant of whether a pitcher will be pulled after a pitch is whether or not it is the last pitch of an inning.  For all of our models it is the most important variable and when we exclude end_inning from them our models perform significantly worse, sometimes worse than a model which doesn’t classify any pitch as a last pitch.  From this we can conclude that baseball managers are far more likely to pull a pitcher at the end of an inning than in the middle of an inning.  There may be some benefit to pulling a pitcher at the end of an inning, such as preventing a reliever from having to cool down between innings.  Cooling down between innings is a significant source of fatigue for pitchers, and relievers are less accustomed to handling this fatigue than starters.  \n",
    "       \n",
    "    However, we believe that this effect is minor compared to the benefit from having better pitching on the field.  As a result, we believe that baseball managers are likely to pull pitchers at a relatively arbitrary time, at the end of an inning. Instead, they should consider focusing on changing pitchers when doing so would result in higher quality pitching.  Identifying the optimal time to change pitchers is outside the scope of this research, but would be an interesting topic to study further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    \n",
    "    Although we didn’t include it in our current model, we’d like to measure the change in variation amongst metrics such as release point and velocity for each pitch type.  Since pitchers aren’t perfect, the velocity and release points for their pitches will always vary.  We hypothesize that as the game goes on, the variability amongst these pitches increases.  Essentially, if a pitcher is displaying erratic an erratic velocity, we’d assume that it was a result of fatigue and he’d be more likely to be pulled by the manager as a result.  We would have to measure this variability for each pitch type (fastball, curveball, changeup, etc) because their velocity and release points differ.  We also code to initialize a SQL database so we can easily explore the data further in the future if we choose to do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphs\n",
    "import matplotlib as plt\n",
    "import csv\n",
    "from graphs import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
